#purpose of this module is to offer up a high level of abstraction for managing bulk download operations
#conceptually this could be part of the session object, but I think it's better not to clutter that further

from .session import SOAPSession
from os import chdir
from csv import reader
from .utilities import log
from .exceptions import SOAPError
import pickle
import csv

class Controller():
	def __init__(self,):
		self.session = SOAPSession()
		
	def sync_ops(self,startdate,enddate,fprefix,optuples,progressdata=None):
		"""Carry out a full set of sync operations.  Can be a new set of instructions or resuming a previous set.
		Takes the arguments:
		startdate - isoformatted start date of the sync window
		enddate - isoformatted end date of the sync window
		fprefix - prefix which will be prepended to the name of all files generated by this operation
		optuples - a list of tuples, of the form (data_element, op, [list, of, fields]) specifying what to download
		progressdata - if applicable, an dictionary object indicating how much of each download has already been completed, of the form {(data_element, op) : pages, ...}"""
		log('Initiating a sync session from %s to %s' % (startdate,enddate))
		self.progress = {}
		try:
			assert progressdata is None
		except AssertionError:
			self.progress=progressdata
			
		try:
			self.session.start_sync(startdate,enddate)
		except SOAPError as e:
			if e.faultcode == 'CLIENT':
				self.session.end_sync()
				self.session.start_sync(startdate,enddate)
			else:
				raise
		for (data_element,op,fields) in optuples:
			records = self.session.getcount(data_element,op)
			pages = (records - records % 100) / 100 + 1
			try:
				page = self.progress.get((data_element,op)) + 1
			except TypeError:
				page = 1
			while page <= pages:
				try:
					self.download_pages(data_element,fields,op,page,min(page+49,pages),fprefix + '-' + data_element + '-' + op + '.csv',newfile=page==1)
					page += 50
				except:
					self.progress[(data_element,op)]+= -1
					with open(fprefix + '_progress.pk3','wb') as progressdump:
						pickle.dump(self.progress,progressdump,protocol=3)
					raise
	
	def sync_from_folder(self,folder):
		"""Designate a folder containing a guidefile containing instructions for download ops.
		Format of the guidefile should be as follows:
		first row: start date (isoformat), end date (isoformat), prefix
		succeeding rows: data_element (Constituent,ActionAlertResponse,etc), operation (insert, update, delete), list of all fields to download"""
		chdir(folder)
		instructions = []
		with open('guidefile.csv','rt') as guidefile:
			guidedata = csv.reader(guidefile)
			[sdate, edate, prefix] = next(guidedata)
			for instruct in guidedata:
				instructions.append([instruct[0],instruct[1],instruct[2:]])
		try:
			with open(prefix + '_progress.pk3','rb') as progressfile:
				progress = pickle.load(progressfile)
		except FileNotFoundError:
			progress = None
		self.sync_ops(sdate, edate, prefix, instructions,progressdata=progress)
		
		
	def download_pages(self,data_element,fields,op,startpage,endpage,destfile,newfile=True):
		if newfile:
			self.session._prep_writefile(destfile)
		else:
			self.session._append_writefile(destfile)
		
		while startpage <= endpage:
			self.session.dl_write(data_element,fields,op,page=startpage)
			startpage += 1
			if startpage % 10 == 0:
				self.session.writefile.flush()
			self.progress[(data_element,op)] = startpage
		log('Downloaded pages %s to %s of %s records from the %s set to %s.' % (str(startpage), str(endpage), data_element, op, destfile))
		